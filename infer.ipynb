{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "capital-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from torch.cuda import amp\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-horizon",
   "metadata": {},
   "source": [
    "# metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "breathing-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_multi_label(output, target, label = 1):\n",
    "    output = output.numpy()\n",
    "    target = target.numpy()\n",
    "\n",
    "    if len(target.shape) == 1:\n",
    "        target = target.astype(int)\n",
    "        n_values = np.max(target) + 1\n",
    "        target = np.eye(n_values)[target]\n",
    "        \n",
    "    # print(target)\n",
    "    return sklearn.metrics.roc_auc_score(target, output, multi_class = 'ovr', average=None)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-latin",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "collected-franchise",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def width_padding(array, desired_w):\n",
    "    w = array.shape[1]\n",
    "    if w > desired_w:\n",
    "        return array[:, :desired_w]\n",
    "    else:\n",
    "        b = (desired_w - w) // 2\n",
    "        bb = desired_w - b - w\n",
    "        return np.pad(array, pad_width=((0, 0), (b, bb)), mode='constant')\n",
    "\n",
    "def crop_or_pad(y, length, is_train=True, start=None):\n",
    "    if len(y) < length:\n",
    "        n_repeats = length // len(y)\n",
    "        epsilon = length % len(y)\n",
    "        \n",
    "        y = np.concatenate([y]*n_repeats + [y[:epsilon]])\n",
    "        \n",
    "    elif len(y) > length:\n",
    "        if not is_train:\n",
    "            start = start or 0\n",
    "        else:\n",
    "            start = start or np.random.randint(len(y) - length)\n",
    "\n",
    "        y = y[start:start + length]\n",
    "\n",
    "    return y\n",
    "\n",
    "# def trim_and_pad(audio, max_samples):\n",
    "#     audio_length = audio.shape[0]\n",
    "#     if audio_length > max_samples:\n",
    "#         # trim long_data\n",
    "#         trim_length = audio_length - max_samples\n",
    "#         audio = audio[int(trim_length//2):int(max_samples+trim_length//2)]\n",
    "#     else:\n",
    "#         # n_repeats = max_samples // len(audio)\n",
    "#         # epsilon = max_samples % len(audio)\n",
    "        \n",
    "#         # audio = np.concatenate([audio]*n_repeats + [audio[:epsilon]])\n",
    "#         padding = int(max_samples - audio_length)\n",
    "#         offset = int(padding // 2)\n",
    "#         audio = np.pad(audio, (offset, max_samples - audio_length - offset), 'constant')\n",
    "    \n",
    "#     return audio\n",
    "\n",
    "def trim_and_pad(audio, max_samples):\n",
    "    audio_length = audio.shape[0]\n",
    "    if audio_length > max_samples:\n",
    "        # trim long_data\n",
    "        trim_length = audio_length - max_samples\n",
    "        audio = audio[int(trim_length//2):int(max_samples+trim_length//2)]\n",
    "    else:\n",
    "        # n_repeats = max_samples // len(audio)\n",
    "        # epsilon = max_samples % len(audio)\n",
    "        \n",
    "        # audio = np.concatenate([audio]*n_repeats + [audio[:epsilon]])\n",
    "        # padding = int(max_samples - audio_length)\n",
    "        # offset = int(padding // 2)\n",
    "        # audio = np.pad(audio, (offset, max_samples - audio_length - offset), 'constant')\n",
    "\n",
    "        if len(audio) < max_samples:\n",
    "            n_repeats = max_samples // len(audio)\n",
    "            epsilon = max_samples % len(audio)\n",
    "            \n",
    "            audio = np.concatenate([audio]*n_repeats + [audio[:epsilon]])\n",
    "    \n",
    "    return audio\n",
    "\n",
    "\n",
    "def segments(audio, fs, segment_size_t=0.05):\n",
    "    audio_len = len(audio)\n",
    "    segment_size = int(segment_size_t * fs)  # segment size in samples\n",
    "    # Break signal into list of segments in a single-line Python code\n",
    "    segments = np.array([audio[x:x + segment_size] for x in\n",
    "                         np.arange(0, audio_len, segment_size)])\n",
    "    return segments\n",
    "\n",
    "def remove_silent(audio, fs, segment_size_t, v2=False):\n",
    "    normalized_segments = segments(audio, fs, segment_size_t)\n",
    "    energies = np.array([(s**2).sum() / len(s) for s in normalized_segments])\n",
    "    threshold = 0.4 * np.median(energies)\n",
    "    index_of_segments_to_keep = (np.where(energies > threshold)[0])\n",
    "    # get segments that have energies higher than a the threshold:\n",
    "    high_energy_segments = normalized_segments[index_of_segments_to_keep]\n",
    "    try:\n",
    "        return np.concatenate(high_energy_segments)\n",
    "    except:\n",
    "        return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "lucky-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_config = {\n",
    "    \"do_remove_silent\": True,\n",
    "    \"n_mfcc\": 15,\n",
    "    \"n_fft\": 1024, \n",
    "    \"hop_length\": 256, \n",
    "    \"target_sr\": 48000, \n",
    "    \"max_duration\": 15\n",
    "}\n",
    "audio_transforms = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-chart",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "affiliated-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimmBackbone(nn.Module):\n",
    "    def __init__(self, model_name, inchannels=3, num_classes=1, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(model_name, in_chans=inchannels, pretrained=pretrained)\n",
    "        n_features = self.backbone.num_features\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(n_features, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "    def freeze(self):\n",
    "        # pass\n",
    "        # print(\"freeze feature_extractor\")\n",
    "        for param in self.backbone.parameters():\n",
    "            param.require_grad = False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        # pass\n",
    "        for param in self.backbone.parameters():\n",
    "            param.require_grad = True\n",
    "\n",
    "    def forward(self, x, fp16=False):\n",
    "        with amp.autocast(enabled=fp16):\n",
    "            x = x.float()\n",
    "            feats = self.backbone.forward_features(x)\n",
    "            x = feat = self.pool(feats).view(x.size(0), -1)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.drop(x)\n",
    "            x = self.fc2(x)\n",
    "        return x, feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-booth",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "bibliographic-plasma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>subject_age</th>\n",
       "      <th>subject_gender</th>\n",
       "      <th>audio_noise_note</th>\n",
       "      <th>cough_intervals</th>\n",
       "      <th>assessment_result</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration_type</th>\n",
       "      <th>subject_age_type</th>\n",
       "      <th>noise_type</th>\n",
       "      <th>cough_interval_type</th>\n",
       "      <th>labels</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9341db3f-049a-4ceb-8438-87ca1618a18a</td>\n",
       "      <td>group_34_48</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'start': 1.9051594202898552, 'end': 2.414901...</td>\n",
       "      <td>0</td>\n",
       "      <td>22050</td>\n",
       "      <td>6.741361</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0_5_1_3_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ff8c21a8-4d05-43d8-96ee-dd33bcd6461e</td>\n",
       "      <td>group_19_33</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'end': 1.516455486542442, 'start': 1.1739296...</td>\n",
       "      <td>0</td>\n",
       "      <td>22050</td>\n",
       "      <td>4.010703</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0_1_1_2_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9a55aef7-ed77-45ab-976e-411aade1c783</td>\n",
       "      <td>group_19_33</td>\n",
       "      <td>male</td>\n",
       "      <td>tiếng nói chuyện, trao đổi</td>\n",
       "      <td>[{'start': 2.393298134459697, 'end': 2.7450841...</td>\n",
       "      <td>0</td>\n",
       "      <td>48000</td>\n",
       "      <td>5.461333</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0_3_0_2_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bf19a7f1-fdff-42c7-a99d-40546b39f745</td>\n",
       "      <td>group_65_78</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'end': 1.47575373406193, 'start': 0.69777486...</td>\n",
       "      <td>1</td>\n",
       "      <td>22050</td>\n",
       "      <td>7.338685</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1_7_1_5_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1c2e03d2-1285-42ed-bca2-b3f1ab94aa6b</td>\n",
       "      <td>group_65_78</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'end': 1.262585034013605, 'start': 0.9351079...</td>\n",
       "      <td>0</td>\n",
       "      <td>48000</td>\n",
       "      <td>2.133333</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0_0_1_5_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4468</th>\n",
       "      <td>8e98b0ef-d164-4fc3-b28b-24f5b70b33c4</td>\n",
       "      <td>group_14_18</td>\n",
       "      <td>female</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'start': 0.8291162349457271, 'end': 1.325240...</td>\n",
       "      <td>0</td>\n",
       "      <td>48000</td>\n",
       "      <td>3.669333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0_1_1_1_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>e7a7732c-0085-499c-9bd8-13c3de35741e</td>\n",
       "      <td>group_19_33</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'start': 1.034520547945205, 'end': 1.2332420...</td>\n",
       "      <td>0</td>\n",
       "      <td>48000</td>\n",
       "      <td>5.546667</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0_3_1_2_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4494</th>\n",
       "      <td>24ac8a4c-b2a2-4dd0-89ac-7ff8801e9558</td>\n",
       "      <td>group_19_33</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'end': 2.5490455813953483, 'start': 1.668881...</td>\n",
       "      <td>0</td>\n",
       "      <td>48000</td>\n",
       "      <td>6.144000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0_4_1_2_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>f744cd1c-8f0d-4043-b800-19e39d443161</td>\n",
       "      <td>group_19_33</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'start': 1.9232463768115942, 'end': 2.529590...</td>\n",
       "      <td>0</td>\n",
       "      <td>48000</td>\n",
       "      <td>12.202667</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0_7_1_2_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500</th>\n",
       "      <td>c936f1a1-f6cf-48e5-82a1-872a58b6fd01</td>\n",
       "      <td>group_19_33</td>\n",
       "      <td>female</td>\n",
       "      <td>không có âm thanh</td>\n",
       "      <td>[{'start': 0.204485339942542, 'end': 0.3904662...</td>\n",
       "      <td>1</td>\n",
       "      <td>22050</td>\n",
       "      <td>0.847574</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1_0_0_2_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>901 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      uuid  subject_age subject_gender  \\\n",
       "1     9341db3f-049a-4ceb-8438-87ca1618a18a  group_34_48           male   \n",
       "3     ff8c21a8-4d05-43d8-96ee-dd33bcd6461e  group_19_33           male   \n",
       "7     9a55aef7-ed77-45ab-976e-411aade1c783  group_19_33           male   \n",
       "21    bf19a7f1-fdff-42c7-a99d-40546b39f745  group_65_78           male   \n",
       "29    1c2e03d2-1285-42ed-bca2-b3f1ab94aa6b  group_65_78           male   \n",
       "...                                    ...          ...            ...   \n",
       "4468  8e98b0ef-d164-4fc3-b28b-24f5b70b33c4  group_14_18         female   \n",
       "4481  e7a7732c-0085-499c-9bd8-13c3de35741e  group_19_33           male   \n",
       "4494  24ac8a4c-b2a2-4dd0-89ac-7ff8801e9558  group_19_33           male   \n",
       "4496  f744cd1c-8f0d-4043-b800-19e39d443161  group_19_33           male   \n",
       "4500  c936f1a1-f6cf-48e5-82a1-872a58b6fd01  group_19_33         female   \n",
       "\n",
       "                audio_noise_note  \\\n",
       "1                           None   \n",
       "3                           None   \n",
       "7     tiếng nói chuyện, trao đổi   \n",
       "21                          None   \n",
       "29                          None   \n",
       "...                          ...   \n",
       "4468                        None   \n",
       "4481                        None   \n",
       "4494                        None   \n",
       "4496                        None   \n",
       "4500           không có âm thanh   \n",
       "\n",
       "                                        cough_intervals  assessment_result  \\\n",
       "1     [{'start': 1.9051594202898552, 'end': 2.414901...                  0   \n",
       "3     [{'end': 1.516455486542442, 'start': 1.1739296...                  0   \n",
       "7     [{'start': 2.393298134459697, 'end': 2.7450841...                  0   \n",
       "21    [{'end': 1.47575373406193, 'start': 0.69777486...                  1   \n",
       "29    [{'end': 1.262585034013605, 'start': 0.9351079...                  0   \n",
       "...                                                 ...                ...   \n",
       "4468  [{'start': 0.8291162349457271, 'end': 1.325240...                  0   \n",
       "4481  [{'start': 1.034520547945205, 'end': 1.2332420...                  0   \n",
       "4494  [{'end': 2.5490455813953483, 'start': 1.668881...                  0   \n",
       "4496  [{'start': 1.9232463768115942, 'end': 2.529590...                  0   \n",
       "4500  [{'start': 0.204485339942542, 'end': 0.3904662...                  1   \n",
       "\n",
       "      sample_rate   duration  duration_type  subject_age_type  noise_type  \\\n",
       "1           22050   6.741361              5                 3           1   \n",
       "3           22050   4.010703              1                 2           1   \n",
       "7           48000   5.461333              3                 2           0   \n",
       "21          22050   7.338685              7                 5           1   \n",
       "29          48000   2.133333              0                 5           1   \n",
       "...           ...        ...            ...               ...         ...   \n",
       "4468        48000   3.669333              1                 1           1   \n",
       "4481        48000   5.546667              3                 2           1   \n",
       "4494        48000   6.144000              4                 2           1   \n",
       "4496        48000  12.202667              7                 2           1   \n",
       "4500        22050   0.847574              0                 2           0   \n",
       "\n",
       "      cough_interval_type     labels  fold  \n",
       "1                       2  0_5_1_3_2   1.0  \n",
       "3                       2  0_1_1_2_2   1.0  \n",
       "7                       2  0_3_0_2_2   1.0  \n",
       "21                      2  1_7_1_5_2   1.0  \n",
       "29                      2  0_0_1_5_2   1.0  \n",
       "...                   ...        ...   ...  \n",
       "4468                    2  0_1_1_1_2   1.0  \n",
       "4481                    2  0_3_1_2_2   1.0  \n",
       "4494                    2  0_4_1_2_2   1.0  \n",
       "4496                    2  0_7_1_2_2   1.0  \n",
       "4500                    2  1_0_0_2_2   1.0  \n",
       "\n",
       "[901 rows x 14 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('/home/hana/sonnh/data/AICovidVN/aicv115m_public_train_full/aicv115m_final_public_train/kfold_multi_feature.csv')\n",
    "df_train = df_train[df_train['fold']==1]\n",
    "df_train\n",
    "\n",
    "# df_train = pd.read_csv('/home/hana/sonnh/data/AICovidVN/virufy-cdf-coughvid/virufy-cdf-coughvid.csv')\n",
    "# df_train = df_train[df_train['pcr_test_result'] == 'positive']\n",
    "# df_train\n",
    "\n",
    "# df_train = pd.read_csv('/home/hana/sonnh/data/AICovidVN/virufy-cdf-india-clinical-1/virufy-cdf-india-clinical-1-full.csv')\n",
    "# df_train['pcr_test_result'] = df_train['pcr_test_result'].fillna('None')\n",
    "# df_train = df_train[df_train['pcr_test_result'] != 'None']\n",
    "# df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "processed-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_dir = '/home/hana/sonnh/data/AICovidVN/aicv115m_public_train_full/aicv115m_final_public_train/train_115M_final_rm_silent_2/'\n",
    "audio_dir = '/home/hana/sonnh/data/AICovidVN/aicv115m_public_train_full/aicv115m_final_public_train/public_train_audio_files/'\n",
    "\n",
    "# audio_dir = '/home/hana/sonnh/data/AICovidVN/virufy-cdf-coughvid/'\n",
    "# audio_dir = '/home/hana/sonnh/data/AICovidVN/virufy-cdf-india-clinical-1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-manner",
   "metadata": {},
   "source": [
    "## mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_feature(audio, fs, mfcc_config, audio_transforms=None, for_test=False):\n",
    "    # n_mfcc=15\n",
    "    # n_fft=1024\n",
    "    # hop_length= 256\n",
    "    # max_samples = int(7.5 * 8000) # 7.5s\n",
    "\n",
    "    # do_remove_silent = mfcc_config.get(\"do_remove_silent\", False)\n",
    "    n_mfcc = mfcc_config.get(\"n_mfcc\", 15)\n",
    "    n_fft = mfcc_config.get(\"n_fft\", 1024)\n",
    "    hop_length = mfcc_config.get(\"hop_length\", 256)\n",
    "    max_duration = mfcc_config.get(\"max_duration\", 15)\n",
    "    target_sr = mfcc_config.get(\"target_sr\", 48000)\n",
    "    max_samples = int(max_duration * target_sr)\n",
    "    if for_test:\n",
    "        # if it's the test set -> do remove silent and  resample\n",
    "        audio = remove_silent(audio, fs, segment_size_t=0.025)\n",
    "        audio = librosa.resample(audio, fs, target_sr)\n",
    "        fs = target_sr\n",
    "    if audio_transforms is not None:\n",
    "        try:\n",
    "            audio, fs = audio_transforms(audio, fs)\n",
    "        except:\n",
    "            audio = audio_transforms(samples=audio, sample_rate=fs)\n",
    "    audio = trim_and_pad(audio, max_samples)\n",
    "    mfcc_feature = librosa.feature.mfcc(y=audio, sr=fs, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    return mfcc_feature[None, ...].astype(np.float64)\n",
    "\n",
    "def extract_feature(audio, fs, segment_size_t=0.025, n_mfcc=26, n_fft=256, hop_length=40, audio_transfroms=None):\n",
    "    # audio = remove_silent(audio, fs, segment_size_t)\n",
    "    if audio_transfroms is not None:\n",
    "        try:\n",
    "            audio, fs = audio_transfroms(audio, fs)\n",
    "        except:\n",
    "            audio = audio_transfroms(samples=audio, sample_rate=fs)\n",
    "    mfcc_feature = librosa.feature.mfcc(y=audio, sr=fs, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    mfcc_delta = librosa.feature.delta(mfcc_feature)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc_delta, order=2)\n",
    "    # zero crossing rate\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=audio, frame_length=n_fft, hop_length=hop_length)\n",
    "    stft = librosa.stft(y=audio, n_fft=n_fft, hop_length= hop_length)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    return np.concatenate([mfcc_feature, mfcc_delta, mfcc_delta2, zcr, stft, chroma_stft])\n",
    "\n",
    "\n",
    "def mfcc_feature(audio, fs, audio_transforms=None):\n",
    "    segment_size_t=0.025\n",
    "    n_mfcc=39\n",
    "    n_fft=256\n",
    "    num_seg = 128\n",
    "    hop_length=len(audio)//num_seg\n",
    "\n",
    "    feature = extract_feature(audio,\n",
    "                             fs,\n",
    "                             segment_size_t,\n",
    "                             n_mfcc,\n",
    "                             n_fft,\n",
    "                             hop_length,\n",
    "                             audio_transfroms=audio_transforms)\n",
    "    # padding or trucate to the same width\n",
    "    feature = width_padding(feature, num_seg)\n",
    "    return feature[None, ...].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "general-reservoir",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'uuid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/sonnh/env/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'uuid'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-14263866d8a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_predicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0muuid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uuid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0maudio_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}/{}.wav'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sonnh/env/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sonnh/env/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'uuid'"
     ]
    }
   ],
   "source": [
    "train_predicts = []\n",
    "for uuid in tqdm(list(df_train['uuid'])):\n",
    "    with torch.no_grad():\n",
    "        audio_path = '{}/{}.wav'.format(audio_dir, uuid)\n",
    "        audio, fs  = sf.read(audio_path, dtype=\"float32\")\n",
    "        audio = remove_silent(audio, fs, segment_size_t=0.025)\n",
    "        audio = librosa.resample(audio, fs, 48000)\n",
    "        audio = np.array(audio)\n",
    "#         sf.write('temp.wav', audio, 48000)\n",
    "#         audio, fs  = sf.read('temp.wav', dtype=\"float32\")\n",
    "        \n",
    "        image = extract_mfcc_feature(audio, fs, mfcc_config, audio_transforms, for_test=False)\n",
    "        image = torch.tensor(image).unsqueeze(0)\n",
    "        image = image.to(device)\n",
    "        predict = model(image)\n",
    "        predict = torch.softmax(predict, dim = 1)\n",
    "    train_predicts.append(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-crazy",
   "metadata": {},
   "source": [
    "## melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "polyphonic-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from  soundfile import SoundFile\n",
    "import glob \n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from skimage.transform import rescale, resize, downscale_local_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "overall-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio2melspec(audio, sr, melspec_config):    \n",
    "    #melspectrogram\n",
    "    # if audio_transfroms is not None:\n",
    "    #     audio, original_sr = audio_transfroms(audio, original_sr)\n",
    "    # n_fft = 512   \n",
    "    # hop_length=int(len(audio)/256)\n",
    "\n",
    "    # mel_spect = librosa.feature.melspectrogram(y=audio, sr=original_sr, n_fft=n_fft, hop_length=hop_length)\n",
    "    # mel_spect = librosa.power_to_db(mel_spect, ref=np.max).astype(np.float32)\n",
    "    # image = mono_to_color(mel_spect)\n",
    "    # # new_img =  cv2.merge([image[:, :128], image[:, 128:256], image[:,256:384]])\n",
    "    # new_img =  cv2.merge([image, image, image])\n",
    "    # new_img = cv2.resize(new_img, (256, 128))\n",
    "\n",
    "    IMAGE_WIDTH = melspec_config.get(\"width\", 448)\n",
    "    IMAGE_HEIGHT = melspec_config.get(\"height\", 448)\n",
    "    n_fft = melspec_config.get(\"n_fft\", 2048)\n",
    "    hop_length = melspec_config.get(\"hop_length\", 'audo')\n",
    "    win_length = n_fft#//2\n",
    "#     sr = melspec_config.get(\"target_sr\", 48000)\n",
    "\n",
    "    if hop_length == 'auto':\n",
    "        hop_length = int((len(audio) - win_length + n_fft) / IMAGE_WIDTH) + 1\n",
    "    else:\n",
    "        # print(hop_length)\n",
    "        raise \"Hop length must be compute with image width\"\n",
    "\n",
    "    spect = np.abs(librosa.stft(y=audio, n_fft=n_fft, hop_length=hop_length, win_length=win_length))\n",
    "    if spect.shape[1] < IMAGE_WIDTH:\n",
    "        #print('too large hop length, len(clip)=', len(clip))\n",
    "        hop_length = hop_length - 1\n",
    "        spect = np.abs(librosa.stft(y=audio, n_fft=n_fft, hop_length=hop_length, win_length=win_length))\n",
    "    if spect.shape[1] > IMAGE_WIDTH:\n",
    "        spect = spect[:, :IMAGE_WIDTH]\n",
    "#     n_mels = IMAGE_HEIGHT // 2\n",
    "    n_mels = IMAGE_HEIGHT\n",
    "    spect = librosa.feature.melspectrogram(S=spect, sr=sr, n_fft=n_fft, n_mels=n_mels, fmin=300, fmax=sr//2)\n",
    "    spect = librosa.power_to_db(spect)\n",
    "    # print(spect.shape)\n",
    "    spect = resize(spect, (IMAGE_HEIGHT, IMAGE_WIDTH), preserve_range=True, anti_aliasing=True)\n",
    "    spect = spect - spect.min()\n",
    "    smax = spect.max()\n",
    "\n",
    "    if smax >= 0.001:\n",
    "        spect = spect / smax\n",
    "    else:\n",
    "        spect[...] = 0\n",
    "\n",
    "    return spect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "refined-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "melspec_config = {\n",
    "    \"width\": 448, \"height\":448, \"n_fft\": 2048, \"hop_length\": \"auto\", \"target_sr\": 48000, \"max_duration\": 15\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "engaging-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "image_transform = albu.Compose([\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "max_samples = 48000 * 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "regulation-response",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 901/901 [02:55<00:00,  5.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_predicts = []\n",
    "list_uuid = list(df_train['uuid'])\n",
    "# list_uuid = list(df_train['cough_path'])\n",
    "for uuid in tqdm(list_uuid):\n",
    "    with torch.no_grad():\n",
    "        audio_path = '{}/{}.wav'.format(audio_dir, uuid)\n",
    "\n",
    "#         audio_path = '{}/{}'.format(audio_dir, uuid)\n",
    "#         audio, fs  = sf.read(audio_path, dtype=\"float32\")\n",
    "        audio, fs = librosa.load(audio_path, sr = None)\n",
    "        audio = remove_silent(audio, fs, segment_size_t=0.025)\n",
    "        audio = librosa.resample(audio, fs, 48000)\n",
    "        audio = np.array(audio)\n",
    "        \n",
    "        sf.write('temp.wav', audio, 48000)\n",
    "        audio, fs  = sf.read('temp.wav', dtype=\"float32\")\n",
    "        \n",
    "        audio = trim_and_pad(audio, max_samples)\n",
    "        image = audio2melspec(audio, fs, melspec_config)\n",
    "        image = image_transform(image = image)['image']\n",
    "        image = torch.tensor(image).unsqueeze(0)\n",
    "        image = image.to(device)\n",
    "        predict, _ = model(image)\n",
    "        predict = torch.softmax(predict, dim = 1)\n",
    "    train_predicts.append(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "expanded-canvas",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-238-272f2ec19028>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# labels = torch.tensor([1 if i == 'positive' else 0 for i in df_train['pcr_test_result']])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_predicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_predicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "labels = torch.tensor(list(df_train['assessment_result']))\n",
    "# labels = torch.tensor([1] * len(df_train))\n",
    "# labels = torch.tensor([1 if i == 'positive' else 0 for i in df_train['pcr_test_result']])\n",
    "\n",
    "train_predicts = torch.cat(train_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "changing-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predicts = train_predicts.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "adult-bryan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(130)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_predicts[:, 1]> 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "athletic-member",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9846658004356632"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(labels, train_predicts[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-logic",
   "metadata": {},
   "source": [
    "## mfcc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "crazy-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def width_padding(array, desired_w):\n",
    "    w = array.shape[1]\n",
    "    if w > desired_w:\n",
    "        return array[:, :desired_w]\n",
    "    else:\n",
    "        b = (desired_w - w) // 2\n",
    "        bb = desired_w - b - w\n",
    "        return np.pad(array, pad_width=((0, 0), (b, bb)), mode='constant')\n",
    "\n",
    "def crop_or_pad(y, length, is_train=True, start=None):\n",
    "    if len(y) < length:\n",
    "        n_repeats = length // len(y)\n",
    "        epsilon = length % len(y)\n",
    "        \n",
    "        y = np.concatenate([y]*n_repeats + [y[:epsilon]])\n",
    "        \n",
    "    elif len(y) > length:\n",
    "        if not is_train:\n",
    "            start = start or 0\n",
    "        else:\n",
    "            start = start or np.random.randint(len(y) - length)\n",
    "\n",
    "        y = y[start:start + length]\n",
    "\n",
    "    return y\n",
    "\n",
    "def trim_and_pad(audio, max_samples):\n",
    "    audio_length = audio.shape[0]\n",
    "    if audio_length > max_samples:\n",
    "        # trim long_data\n",
    "        trim_length = audio_length - max_samples\n",
    "        audio = audio[int(trim_length//2):int(max_samples+trim_length//2)]\n",
    "    else:\n",
    "        # n_repeats = max_samples // len(audio)\n",
    "        # epsilon = max_samples % len(audio)\n",
    "        \n",
    "        # audio = np.concatenate([audio]*n_repeats + [audio[:epsilon]])\n",
    "        padding = int(max_samples - audio_length)\n",
    "        offset = int(padding // 2)\n",
    "        audio = np.pad(audio, (offset, max_samples - audio_length - offset), 'constant')\n",
    "    \n",
    "    return audio\n",
    "\n",
    "def padding_repeat(audio, max_samples):\n",
    "    audio_length = audio.shape[0]\n",
    "    if audio_length > max_samples:\n",
    "        # trim long_data\n",
    "        trim_length = audio_length - max_samples\n",
    "        new_audio = audio[int(trim_length//2):int(max_samples+trim_length//2)]\n",
    "    else:\n",
    "        # n_repeats = max_samples // len(audio)\n",
    "        # epsilon = max_samples % len(audio)\n",
    "        n_repeats = int(max_samples/audio_length)\n",
    "        new_audio = np.empty(max_samples)\n",
    "        new_audio[:audio_length*n_repeats] = np.tile(audio, n_repeats)\n",
    "        remain = max_samples - n_repeats*audio_length\n",
    "        new_audio[audio_length*n_repeats:] = audio[:remain]\n",
    "    \n",
    "    return new_audio\n",
    "\n",
    "def random_crop(audio, max_length):\n",
    "    len_y = audio.shape[0]\n",
    "    if len_y < max_length:\n",
    "        # audio = padding_repeat(audio, max_length)\n",
    "        new_y = np.zeros(max_length, dtype=audio.dtype)\n",
    "        start = np.random.randint(max_length - len_y)\n",
    "        new_y[start:start + len_y] = audio\n",
    "        audio = new_y.astype(np.float32)\n",
    "    elif len_y > max_length:\n",
    "        start = np.random.randint(len_y - max_length)\n",
    "        audio = audio[start:start + max_length].astype(np.float32)\n",
    "    else:\n",
    "        audio = audio.astype(np.float32)\n",
    "    return audio\n",
    "\n",
    "def segments(audio, fs, segment_size_t=0.05):\n",
    "    audio_len = len(audio)\n",
    "    segment_size = int(segment_size_t * fs)  # segment size in samples\n",
    "    # Break signal into list of segments in a single-line Python code\n",
    "    segments = np.array([audio[x:x + segment_size] for x in\n",
    "                         np.arange(0, audio_len, segment_size)])\n",
    "    return segments\n",
    "\n",
    "def remove_silent(audio, fs, segment_size_t, v2=False):\n",
    "    normalized_segments = segments(audio, fs, segment_size_t)\n",
    "    energies = np.array([(s**2).sum() / len(s) for s in normalized_segments])\n",
    "    threshold = 0.4 * np.median(energies)\n",
    "    index_of_segments_to_keep = (np.where(energies > threshold)[0])\n",
    "    # get segments that have energies higher than a the threshold:\n",
    "    high_energy_segments = normalized_segments[index_of_segments_to_keep]\n",
    "    try:\n",
    "        return np.concatenate(high_energy_segments)\n",
    "    except:\n",
    "        return audio\n",
    "\n",
    "def scale_minmax(X, min=0.0, max=1.0):\n",
    "    X_std = (X - X.min()) / (X.max() - X.min())\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    return X_scaled\n",
    "\n",
    "def extract_mfcc_feature2(audio, fs, mfcc_config):\n",
    "\n",
    "    n_mfcc = mfcc_config.get(\"n_mfcc\", 39)\n",
    "    n_fft = mfcc_config.get(\"n_fft\", 1024)\n",
    "    hop_length = mfcc_config.get(\"hop_length\", 256)\n",
    "    max_duration = mfcc_config.get(\"max_duration\", 10)\n",
    "    target_sr = mfcc_config.get(\"target_sr\", 22050)\n",
    "    max_samples = int(max_duration * target_sr)\n",
    "    \n",
    "    print(max_samples)\n",
    "    \n",
    "    mfcc_feature = librosa.feature.mfcc(y=audio, sr=fs, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    if mfcc_config.get(\"use_derivative\", False):\n",
    "        mfcc_delta = librosa.feature.delta(mfcc_feature)\n",
    "        mfcc_delta2 = librosa.feature.delta(mfcc_feature, order=2)\n",
    "        mfcc_feature = np.concatenate([mfcc_feature, mfcc_delta, mfcc_delta2])\n",
    "    if mfcc_config.get(\"normalize\", False):\n",
    "        mfcc_feature = scale_minmax(mfcc_feature, 0, 255).astype(np.uint8)\n",
    "        mfcc_feature = np.flip(mfcc_feature, axis=0) # put low frequencies at the bottom in image\n",
    "        mfcc_feature = 255-mfcc_feature # invert. make black==more energy\n",
    "        mfcc_feature = mfcc_feature / 255.0\n",
    "    return mfcc_feature[None, ...].astype(np.float64)\n",
    "\n",
    "def extract_feature(audio, fs, segment_size_t=0.025, n_mfcc=26, n_fft=256, hop_length=40, audio_transfroms=None):\n",
    "    # audio = remove_silent(audio, fs, segment_size_t)\n",
    "    if audio_transfroms is not None:\n",
    "        try:\n",
    "            audio, fs = audio_transfroms(audio, fs)\n",
    "        except:\n",
    "            audio = audio_transfroms(samples=audio, sample_rate=fs)\n",
    "    mfcc_feature = librosa.feature.mfcc(y=audio, sr=fs, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    mfcc_delta = librosa.feature.delta(mfcc_feature)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc_delta, order=2)\n",
    "    # zero crossing rate\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=audio, frame_length=n_fft, hop_length=hop_length)\n",
    "    stft = librosa.stft(y=audio, n_fft=n_fft, hop_length= hop_length)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    return np.concatenate([mfcc_feature, mfcc_delta, mfcc_delta2, zcr, stft, chroma_stft])\n",
    "\n",
    "\n",
    "def mfcc_feature(audio, fs, audio_transforms=None):\n",
    "    segment_size_t=0.025\n",
    "    n_mfcc=39\n",
    "    n_fft=256\n",
    "    num_seg = 128\n",
    "    hop_length=len(audio)//num_seg\n",
    "\n",
    "    feature = extract_feature(audio,\n",
    "                             fs,\n",
    "                             segment_size_t,\n",
    "                             n_mfcc,\n",
    "                             n_fft,\n",
    "                             hop_length,\n",
    "                             audio_transfroms=audio_transforms)\n",
    "    # padding or trucate to the same width\n",
    "    feature = width_padding(feature, num_seg)\n",
    "    return feature[None, ...].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "filled-execution",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.modules.conv as conv\n",
    "\n",
    "\n",
    "class AddCoords(nn.Module):\n",
    "    def __init__(self, rank, with_r=False, use_cuda=True):\n",
    "        super(AddCoords, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.with_r = with_r\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        :param input_tensor: shape (N, C_in, H, W)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.rank == 1:\n",
    "            batch_size_shape, channel_in_shape, dim_x = input_tensor.shape\n",
    "            xx_range = torch.arange(dim_x, dtype=torch.int32)\n",
    "            xx_channel = xx_range[None, None, :]\n",
    "\n",
    "            xx_channel = xx_channel.float() / (dim_x - 1)\n",
    "            xx_channel = xx_channel * 2 - 1\n",
    "            xx_channel = xx_channel.repeat(batch_size_shape, 1, 1)\n",
    "\n",
    "            if torch.cuda.is_available() and self.use_cuda:\n",
    "                input_tensor = input_tensor.cuda()\n",
    "                xx_channel = xx_channel.cuda()\n",
    "            out = torch.cat([input_tensor, xx_channel], dim=1)\n",
    "\n",
    "            if self.with_r:\n",
    "                rr = torch.sqrt(torch.pow(xx_channel - 0.5, 2))\n",
    "                out = torch.cat([out, rr], dim=1)\n",
    "\n",
    "        elif self.rank == 2:\n",
    "            batch_size_shape, channel_in_shape, dim_y, dim_x = input_tensor.shape\n",
    "            xx_ones = torch.ones([1, 1, 1, dim_x], dtype=torch.int32)\n",
    "            yy_ones = torch.ones([1, 1, 1, dim_y], dtype=torch.int32)\n",
    "\n",
    "            xx_range = torch.arange(dim_y, dtype=torch.int32)\n",
    "            yy_range = torch.arange(dim_x, dtype=torch.int32)\n",
    "            xx_range = xx_range[None, None, :, None]\n",
    "            yy_range = yy_range[None, None, :, None]\n",
    "\n",
    "            xx_channel = torch.matmul(xx_range, xx_ones)\n",
    "            yy_channel = torch.matmul(yy_range, yy_ones)\n",
    "\n",
    "            # transpose y\n",
    "            yy_channel = yy_channel.permute(0, 1, 3, 2)\n",
    "\n",
    "            xx_channel = xx_channel.float() / (dim_y - 1)\n",
    "            yy_channel = yy_channel.float() / (dim_x - 1)\n",
    "\n",
    "            xx_channel = xx_channel * 2 - 1\n",
    "            yy_channel = yy_channel * 2 - 1\n",
    "\n",
    "            xx_channel = xx_channel.repeat(batch_size_shape, 1, 1, 1)\n",
    "            yy_channel = yy_channel.repeat(batch_size_shape, 1, 1, 1)\n",
    "\n",
    "            if torch.cuda.is_available() and self.use_cuda:\n",
    "                input_tensor = input_tensor.cuda()\n",
    "                xx_channel = xx_channel.cuda()\n",
    "                yy_channel = yy_channel.cuda()\n",
    "            out = torch.cat([input_tensor, xx_channel, yy_channel], dim=1)\n",
    "\n",
    "            if self.with_r:\n",
    "                rr = torch.sqrt(torch.pow(xx_channel - 0.5, 2) + torch.pow(yy_channel - 0.5, 2))\n",
    "                out = torch.cat([out, rr], dim=1)\n",
    "\n",
    "        elif self.rank == 3:\n",
    "            batch_size_shape, channel_in_shape, dim_z, dim_y, dim_x = input_tensor.shape\n",
    "            xx_ones = torch.ones([1, 1, 1, 1, dim_x], dtype=torch.int32)\n",
    "            yy_ones = torch.ones([1, 1, 1, 1, dim_y], dtype=torch.int32)\n",
    "            zz_ones = torch.ones([1, 1, 1, 1, dim_z], dtype=torch.int32)\n",
    "\n",
    "            xy_range = torch.arange(dim_y, dtype=torch.int32)\n",
    "            xy_range = xy_range[None, None, None, :, None]\n",
    "\n",
    "            yz_range = torch.arange(dim_z, dtype=torch.int32)\n",
    "            yz_range = yz_range[None, None, None, :, None]\n",
    "\n",
    "            zx_range = torch.arange(dim_x, dtype=torch.int32)\n",
    "            zx_range = zx_range[None, None, None, :, None]\n",
    "\n",
    "            xy_channel = torch.matmul(xy_range, xx_ones)\n",
    "            xx_channel = torch.cat([xy_channel + i for i in range(dim_z)], dim=2)\n",
    "            xx_channel = xx_channel.repeat(batch_size_shape, 1, 1, 1, 1)\n",
    "\n",
    "            yz_channel = torch.matmul(yz_range, yy_ones)\n",
    "            yz_channel = yz_channel.permute(0, 1, 3, 4, 2)\n",
    "            yy_channel = torch.cat([yz_channel + i for i in range(dim_x)], dim=4)\n",
    "            yy_channel = yy_channel.repeat(batch_size_shape, 1, 1, 1, 1)\n",
    "\n",
    "            zx_channel = torch.matmul(zx_range, zz_ones)\n",
    "            zx_channel = zx_channel.permute(0, 1, 4, 2, 3)\n",
    "            zz_channel = torch.cat([zx_channel + i for i in range(dim_y)], dim=3)\n",
    "            zz_channel = zz_channel.repeat(batch_size_shape, 1, 1, 1, 1)\n",
    "\n",
    "            if torch.cuda.is_available() and self.use_cuda:\n",
    "                input_tensor = input_tensor.cuda()\n",
    "                xx_channel = xx_channel.cuda()\n",
    "                yy_channel = yy_channel.cuda()\n",
    "                zz_channel = zz_channel.cuda()\n",
    "            out = torch.cat([input_tensor, xx_channel, yy_channel, zz_channel], dim=1)\n",
    "\n",
    "            if self.with_r:\n",
    "                rr = torch.sqrt(torch.pow(xx_channel - 0.5, 2) +\n",
    "                                torch.pow(yy_channel - 0.5, 2) +\n",
    "                                torch.pow(zz_channel - 0.5, 2))\n",
    "                out = torch.cat([out, rr], dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class CoordConv1d(conv.Conv1d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True, with_r=False, use_cuda=True):\n",
    "        super(CoordConv1d, self).__init__(in_channels, out_channels, kernel_size,\n",
    "                                          stride, padding, dilation, groups, bias)\n",
    "        self.rank = 1\n",
    "        self.addcoords = AddCoords(self.rank, with_r, use_cuda=use_cuda)\n",
    "        self.conv = nn.Conv1d(in_channels + self.rank + int(with_r), out_channels,\n",
    "                              kernel_size, stride, padding, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        input_tensor_shape: (N, C_in,H,W)\n",
    "        output_tensor_shape: N,C_out,H_out,W_out）\n",
    "        :return: CoordConv2d Result\n",
    "        \"\"\"\n",
    "        out = self.addcoords(input_tensor)\n",
    "        out = self.conv(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class CoordConv2d(conv.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True, with_r=False, use_cuda=True):\n",
    "        super(CoordConv2d, self).__init__(in_channels, out_channels, kernel_size,\n",
    "                                          stride, padding, dilation, groups, bias)\n",
    "        self.rank = 2\n",
    "        self.addcoords = AddCoords(self.rank, with_r, use_cuda=use_cuda)\n",
    "        self.conv = nn.Conv2d(in_channels + self.rank + int(with_r), out_channels,\n",
    "                              kernel_size, stride, padding, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        input_tensor_shape: (N, C_in,H,W)\n",
    "        output_tensor_shape: N,C_out,H_out,W_out）\n",
    "        :return: CoordConv2d Result\n",
    "        \"\"\"\n",
    "        out = self.addcoords(input_tensor)\n",
    "        out = self.conv(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class CoordConv3d(conv.Conv3d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True, with_r=False, use_cuda=True):\n",
    "        super(CoordConv3d, self).__init__(in_channels, out_channels, kernel_size,\n",
    "                                          stride, padding, dilation, groups, bias)\n",
    "        self.rank = 3\n",
    "        self.addcoords = AddCoords(self.rank, with_r, use_cuda=use_cuda)\n",
    "        self.conv = nn.Conv3d(in_channels + self.rank + int(with_r), out_channels,\n",
    "                              kernel_size, stride, padding, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        input_tensor_shape: (N, C_in,H,W)\n",
    "        output_tensor_shape: N,C_out,H_out,W_out）\n",
    "        :return: CoordConv2d Result\n",
    "        \"\"\"\n",
    "        out = self.addcoords(input_tensor)\n",
    "        out = self.conv(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class PlainCNN(nn.Module):\n",
    "    def __init__(self, inchannels=3, num_classes=1, use_coord=False, pretrained=True):\n",
    "        super().__init__()\n",
    "        if use_coord:\n",
    "            self.conv1 = CoordConv2d(inchannels,\n",
    "                                    out_channels=64,\n",
    "                                    kernel_size=(3, 3),\n",
    "                                    stride=(1, 1),\n",
    "                                    padding=(1, 1),\n",
    "                                    with_r=True)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels=inchannels, \n",
    "                                out_channels=64,\n",
    "                                kernel_size=(3, 3), stride=(1, 1),\n",
    "                                padding=(1, 1))\n",
    "        self.act0 = nn.ReLU()\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, \n",
    "                              out_channels=256,\n",
    "                              kernel_size=(2, 2), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(256, 256, bias=True)\n",
    "        self.fc2 = nn.Linear(256, 128, bias=True)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x, fp16=False):\n",
    "        with amp.autocast(enabled=fp16):\n",
    "            x = x.float()\n",
    "            x = self.conv1(x)\n",
    "            x = self.act0(x)\n",
    "\n",
    "            x = self.max_pool1(x)\n",
    "\n",
    "            x = self.conv2(x)\n",
    "            x = self.act1(x)\n",
    "\n",
    "            x = self.bn1(x)\n",
    "            \n",
    "            x = self.pool(x).view(x.size(0), -1)\n",
    "            #x = self.flatten(x)\n",
    "\n",
    "            x = F.relu(self.fc1(x))\n",
    "\n",
    "            x = self.drop1(x)\n",
    "\n",
    "            x = F.relu(self.fc2(x))\n",
    "\n",
    "            x = self.drop2(x)\n",
    "\n",
    "            x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "middle-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='PlainCNN'\n",
    "inchannels=1\n",
    "num_classes=1\n",
    "pretrained=False\n",
    "use_coord = True\n",
    "\n",
    "model = PlainCNN(inchannels=inchannels, num_classes=num_classes, pretrained=pretrained, use_coord = use_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "short-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_paths = []\n",
    "checkpoint_path = '/home/hana/sonnh/covidaivn/Covid19_Cough_Classification/saved/models/18-Covid19-PlainCNN/0805_201606/checkpoint_14_fold1.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "private-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(checkpoint_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fundamental-referral",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "numeric-coffee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>assessment_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iV3Db6t1T8b7c5HQY2TwxIhjbzD3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AxuYWBN0jFVLINCBqIW5aZmGCdu1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C5eIsssb9GSkaAgIfsHMHeR6fSh1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YjbEAECMBIaZKyfqOvWy5DDImUb2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aGOvk4ji0cVqIzCs1jHnzlw2UEy2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           uuid  assessment_result\n",
       "0  iV3Db6t1T8b7c5HQY2TwxIhjbzD3                  0\n",
       "1  AxuYWBN0jFVLINCBqIW5aZmGCdu1                  0\n",
       "2  C5eIsssb9GSkaAgIfsHMHeR6fSh1                  0\n",
       "3  YjbEAECMBIaZKyfqOvWy5DDImUb2                  0\n",
       "4  aGOvk4ji0cVqIzCs1jHnzlw2UEy2                  0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('/home/hana/sonnh/data/AICovidVN/coswara/info.csv')\n",
    "audio_dir = '/home/hana/sonnh/data/AICovidVN/coswara/audio/'\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "attractive-denial",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1796 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154350\n",
      "(1, 39, 302)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_predicts = []\n",
    "mfcc_config = {\n",
    "    \"normalize\": False,\n",
    "    \"use_derivative\": False,\n",
    "    \"n_mfcc\": 39,\n",
    "    \"n_fft\": 2048,\n",
    "    \"hop_length\": 512,\n",
    "    \"target_sr\": 22050,\n",
    "    \"max_duration\": 7\n",
    "}\n",
    "max_samples = mfcc_config['target_sr'] * mfcc_config['max_duration']\n",
    "for uuid in tqdm(list(df_train['uuid'])):\n",
    "    with torch.no_grad():\n",
    "        audio_path = '{}/{}.wav'.format(audio_dir, uuid)\n",
    "        audio, sr  = librosa.load(audio_path, sr = mfcc_config['target_sr'])\n",
    "#         audio, sr = sf.read(audio_path, dtype=\"float32\")\n",
    "#         audio = librosa.resample(audio, sr, 48000)\n",
    "        audio = padding_repeat(audio, max_samples)\n",
    "        \n",
    "        image = extract_mfcc_feature2(audio, sr, mfcc_config)\n",
    "        print(image.shape)\n",
    "        break\n",
    "#         image = torch.tensor(image).unsqueeze(0)\n",
    "#         image = image.to(device)\n",
    "#         predict = model(image)\n",
    "#         predict = torch.sigmoid(predict)[0]\n",
    "#     train_predicts.append(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-district",
   "metadata": {},
   "source": [
    "## metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "married-homework",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = torch.tensor(list(df_train['assessment_result']))\n",
    "# labels = torch.tensor([1] * len(df_train))\n",
    "# labels = torch.tensor([1 if i == 'positive' else 0 for i in df_train['pcr_test_result']])\n",
    "\n",
    "train_predicts = torch.cat(train_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "normal-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predicts = train_predicts.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "constitutional-sudan",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.470178948433016"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(labels, train_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "prescription-heavy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_predicts>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "proper-cable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(labels).count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "pacific-copying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1598"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(labels).count(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-creativity",
   "metadata": {},
   "source": [
    "# test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "statewide-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_test_dir = '/home/hana/sonnh/data/AICovidVN/aicv115m_public_train_full/aicv115m_final_public_test/public_test_audio_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "automatic-possible",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>assessment_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7b3797b0-3b7e-41e3-8b28-e2717eb55f8b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0c466b3-7bf2-47e4-9e7f-f8cfc1783764</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a2d668e9-d876-4bf6-bcb3-0cc32ba20c84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0edbea61-da70-44a4-8ee8-3681027944a6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1bcee200-1c33-4293-b1e9-5854210d92e8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>ab3f935b-3056-4a28-aa88-5823cfb0d30d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>bd8f4a34-33aa-40ad-a390-eb6bc9f04475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>ba5f136a-6c8e-4671-8d1a-5aeaf217bbc7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>634af752-a1ae-424d-b14c-cb2950950cac</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>2efb787e-9806-4059-bca4-d78a001c12a5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1233 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      uuid  assessment_result\n",
       "0     7b3797b0-3b7e-41e3-8b28-e2717eb55f8b                  0\n",
       "1     f0c466b3-7bf2-47e4-9e7f-f8cfc1783764                  0\n",
       "2     a2d668e9-d876-4bf6-bcb3-0cc32ba20c84                  0\n",
       "3     0edbea61-da70-44a4-8ee8-3681027944a6                  0\n",
       "4     1bcee200-1c33-4293-b1e9-5854210d92e8                  0\n",
       "...                                    ...                ...\n",
       "1228  ab3f935b-3056-4a28-aa88-5823cfb0d30d                  0\n",
       "1229  bd8f4a34-33aa-40ad-a390-eb6bc9f04475                  0\n",
       "1230  ba5f136a-6c8e-4671-8d1a-5aeaf217bbc7                  0\n",
       "1231  634af752-a1ae-424d-b14c-cb2950950cac                  0\n",
       "1232  2efb787e-9806-4059-bca4-d78a001c12a5                  0\n",
       "\n",
       "[1233 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('/home/hana/sonnh/data/AICovidVN/aicv115m_public_train_full/aicv115m_final_public_test/public_test_sample_submission.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "satisfactory-issue",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1233/1233 [04:50<00:00,  4.25it/s]\n"
     ]
    }
   ],
   "source": [
    "predicts = []\n",
    "for uuid in tqdm(list(df_test['uuid'])):\n",
    "    with torch.no_grad():\n",
    "        audio_path = '{}/{}.wav'.format(audio_test_dir, uuid)\n",
    "        audio, fs  = sf.read(audio_path, dtype=\"float32\")\n",
    "        audio = remove_silent(audio, fs, segment_size_t=0.025)\n",
    "        audio = librosa.resample(audio, fs, 48000)\n",
    "        \n",
    "        sf.write('temp.wav', audio, 48000)\n",
    "        audio, fs  = sf.read('temp.wav', dtype=\"float32\")\n",
    "\n",
    "        image = extract_mfcc_feature(audio, fs, mfcc_config, audio_transforms, for_test=False)\n",
    "        image = torch.tensor(image).unsqueeze(0)\n",
    "        image = image.to(device)\n",
    "        predict = model(image)\n",
    "        predict = torch.softmax(predict, dim = 1)\n",
    "    predicts.append(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-husband",
   "metadata": {},
   "source": [
    "## melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "loaded-review",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1233 [00:00<?, ?it/s]/home/hana/sonnh/env/lib/python3.6/site-packages/librosa/filters.py:239: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  \"Empty filters detected in mel frequency basis. \"\n",
      "/home/hana/sonnh/env/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 1233/1233 [04:28<00:00,  4.60it/s]\n"
     ]
    }
   ],
   "source": [
    "predicts = []\n",
    "for uuid in tqdm(list(df_test['uuid'])):\n",
    "    with torch.no_grad():\n",
    "        audio_path = '{}/{}.wav'.format(audio_test_dir, uuid)\n",
    "        audio, fs  = sf.read(audio_path, dtype=\"float32\")\n",
    "        audio = remove_silent(audio, fs, segment_size_t=0.025)\n",
    "        audio = librosa.resample(audio, fs, 48000)\n",
    "        audio = np.array(audio)\n",
    "        \n",
    "        \n",
    "        sf.write('temp.wav', audio, 48000)\n",
    "        audio, fs  = sf.read('temp.wav', dtype=\"float32\")\n",
    "        \n",
    "        audio = trim_and_pad(audio, max_samples)\n",
    "        image = audio2melspec(audio, fs, melspec_config)\n",
    "        image = image_transform(image = image)['image']\n",
    "        image = torch.tensor(image).unsqueeze(0)\n",
    "        image = image.to(device)\n",
    "        predict = model(image)\n",
    "        predict = torch.softmax(predict, dim = 1)\n",
    "    predicts.append(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "interior-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = torch.cat(predicts).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "european-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predicts = predicts[:, 1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "treated-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['assessment_result'] = final_predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "catholic-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('/home/hana/sonnh/covid19_res/11_b0_fold1_fix_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "external-richmond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asilla/sonnh/covid\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "connected-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predicts = np.array(final_predicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "polished-suggestion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final_predicts>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "athletic-antenna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>assessment_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7b3797b0-3b7e-41e3-8b28-e2717eb55f8b</td>\n",
       "      <td>0.194483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0c466b3-7bf2-47e4-9e7f-f8cfc1783764</td>\n",
       "      <td>0.020574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a2d668e9-d876-4bf6-bcb3-0cc32ba20c84</td>\n",
       "      <td>0.016501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0edbea61-da70-44a4-8ee8-3681027944a6</td>\n",
       "      <td>0.013703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1bcee200-1c33-4293-b1e9-5854210d92e8</td>\n",
       "      <td>0.022322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid  assessment_result\n",
       "0  7b3797b0-3b7e-41e3-8b28-e2717eb55f8b           0.194483\n",
       "1  f0c466b3-7bf2-47e4-9e7f-f8cfc1783764           0.020574\n",
       "2  a2d668e9-d876-4bf6-bcb3-0cc32ba20c84           0.016501\n",
       "3  0edbea61-da70-44a4-8ee8-3681027944a6           0.013703\n",
       "4  1bcee200-1c33-4293-b1e9-5854210d92e8           0.022322"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "turkish-pacific",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hana/sonnh/data/AICovidVN/aicv115m_public_train_full/aicv115m_final_public_train/public_train_audio_files//f616cb8d-370f-43e6-b459-121a9b987c94.wav'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "genuine-anniversary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0000000e+00 3.0517578e-05 3.0517578e-05 ... 5.7983398e-04 6.1035156e-04\n",
      " 6.1035156e-04]\n"
     ]
    }
   ],
   "source": [
    "audio_path = '{}/{}.wav'.format(audio_test_dir, uuid)\n",
    "audio, fs  = sf.read(audio_path, dtype=\"float32\")\n",
    "audio = remove_silent(audio, fs, segment_size_t=0.025)\n",
    "audio = librosa.resample(audio, fs, 48000)\n",
    "\n",
    "print(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "familiar-snapshot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 3.0517578e-05, 3.0517578e-05, ..., 5.7983398e-04,\n",
       "       6.1035156e-04, 6.1035156e-04], dtype=float32)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "dependent-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write('temp.wav', audio, 48000)\n",
    "audio, fs  = sf.read('temp.wav', dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-gardening",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

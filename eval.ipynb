{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forced-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from torch.cuda import amp\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-berlin",
   "metadata": {},
   "source": [
    "# metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "toxic-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_multi_label(output, target, label = 1):\n",
    "    output = output.numpy()\n",
    "    target = target.numpy()\n",
    "\n",
    "    if len(target.shape) == 1:\n",
    "        target = target.astype(int)\n",
    "        n_values = np.max(target) + 1\n",
    "        target = np.eye(n_values)[target]\n",
    "        \n",
    "    # print(target)\n",
    "    return sklearn.metrics.roc_auc_score(target, output, multi_class = 'ovr', average=None)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-interim",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "described-joyce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def width_padding(array, desired_w):\n",
    "    w = array.shape[1]\n",
    "    if w > desired_w:\n",
    "        return array[:, :desired_w]\n",
    "    else:\n",
    "        b = (desired_w - w) // 2\n",
    "        bb = desired_w - b - w\n",
    "        return np.pad(array, pad_width=((0, 0), (b, bb)), mode='constant')\n",
    "\n",
    "def crop_or_pad(y, length, is_train=True, start=None):\n",
    "    if len(y) < length:\n",
    "        n_repeats = length // len(y)\n",
    "        epsilon = length % len(y)\n",
    "        \n",
    "        y = np.concatenate([y]*n_repeats + [y[:epsilon]])\n",
    "        \n",
    "    elif len(y) > length:\n",
    "        if not is_train:\n",
    "            start = start or 0\n",
    "        else:\n",
    "            start = start or np.random.randint(len(y) - length)\n",
    "\n",
    "        y = y[start:start + length]\n",
    "\n",
    "    return y\n",
    "\n",
    "def trim_and_pad(audio, max_samples):\n",
    "    audio_length = audio.shape[0]\n",
    "    if audio_length > max_samples:\n",
    "        # trim long_data\n",
    "        trim_length = audio_length - max_samples\n",
    "        audio = audio[int(trim_length//2):int(max_samples+trim_length//2)]\n",
    "    else:\n",
    "        # n_repeats = max_samples // len(audio)\n",
    "        # epsilon = max_samples % len(audio)\n",
    "        \n",
    "        # audio = np.concatenate([audio]*n_repeats + [audio[:epsilon]])\n",
    "        # padding = int(max_samples - audio_length)\n",
    "        # offset = int(padding // 2)\n",
    "        # audio = np.pad(audio, (offset, max_samples - audio_length - offset), 'constant')\n",
    "\n",
    "#         if len(audio) < max_samples:\n",
    "#             n_repeats = max_samples // len(audio)\n",
    "#             epsilon = max_samples % len(audio)\n",
    "            \n",
    "#             audio = np.concatenate([audio]*n_repeats + [audio[:epsilon]])\n",
    "    \n",
    "    return audio\n",
    "\n",
    "\n",
    "def segments(audio, fs, segment_size_t=0.05):\n",
    "    audio_len = len(audio)\n",
    "    segment_size = int(segment_size_t * fs)  # segment size in samples\n",
    "    # Break signal into list of segments in a single-line Python code\n",
    "    segments = np.array([audio[x:x + segment_size] for x in\n",
    "                         np.arange(0, audio_len, segment_size)])\n",
    "    return segments\n",
    "\n",
    "def remove_silent(audio, fs, segment_size_t, v2=False):\n",
    "    normalized_segments = segments(audio, fs, segment_size_t)\n",
    "    energies = np.array([(s**2).sum() / len(s) for s in normalized_segments])\n",
    "    threshold = 0.4 * np.median(energies)\n",
    "    index_of_segments_to_keep = (np.where(energies > threshold)[0])\n",
    "    # get segments that have energies higher than a the threshold:\n",
    "    high_energy_segments = normalized_segments[index_of_segments_to_keep]\n",
    "    try:\n",
    "        return np.concatenate(high_energy_segments)\n",
    "    except:\n",
    "        return audio\n",
    "\n",
    "def extract_mfcc_feature(audio, fs, mfcc_config, audio_transforms=None, for_test=False):\n",
    "    # n_mfcc=15\n",
    "    # n_fft=1024\n",
    "    # hop_length= 256\n",
    "    # max_samples = int(7.5 * 8000) # 7.5s\n",
    "\n",
    "    # do_remove_silent = mfcc_config.get(\"do_remove_silent\", False)\n",
    "    n_mfcc = mfcc_config.get(\"n_mfcc\", 15)\n",
    "    n_fft = mfcc_config.get(\"n_fft\", 1024)\n",
    "    hop_length = mfcc_config.get(\"hop_length\", 256)\n",
    "    max_duration = mfcc_config.get(\"max_duration\", 15)\n",
    "    target_sr = mfcc_config.get(\"target_sr\", 48000)\n",
    "    max_samples = int(max_duration * target_sr)\n",
    "    if for_test:\n",
    "        # if it's the test set -> do remove silent and  resample\n",
    "        audio = remove_silent(audio, fs, segment_size_t=0.025)\n",
    "        audio = librosa.resample(audio, fs, target_sr)\n",
    "        fs = target_sr\n",
    "    if audio_transforms is not None:\n",
    "        try:\n",
    "            audio, fs = audio_transforms(audio, fs)\n",
    "        except:\n",
    "            audio = audio_transforms(samples=audio, sample_rate=fs)\n",
    "    audio = trim_and_pad(audio, max_samples)\n",
    "    mfcc_feature = librosa.feature.mfcc(y=audio, sr=fs, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    return mfcc_feature[None, ...].astype(np.float64)\n",
    "\n",
    "def extract_feature(audio, fs, segment_size_t=0.025, n_mfcc=26, n_fft=256, hop_length=40, audio_transfroms=None):\n",
    "    # audio = remove_silent(audio, fs, segment_size_t)\n",
    "    if audio_transfroms is not None:\n",
    "        try:\n",
    "            audio, fs = audio_transfroms(audio, fs)\n",
    "        except:\n",
    "            audio = audio_transfroms(samples=audio, sample_rate=fs)\n",
    "    mfcc_feature = librosa.feature.mfcc(y=audio, sr=fs, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    mfcc_delta = librosa.feature.delta(mfcc_feature)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc_delta, order=2)\n",
    "    # zero crossing rate\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=audio, frame_length=n_fft, hop_length=hop_length)\n",
    "    stft = librosa.stft(y=audio, n_fft=n_fft, hop_length= hop_length)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    return np.concatenate([mfcc_feature, mfcc_delta, mfcc_delta2, zcr, stft, chroma_stft])\n",
    "\n",
    "\n",
    "def mfcc_feature(audio, fs, audio_transforms=None):\n",
    "    segment_size_t=0.025\n",
    "    n_mfcc=39\n",
    "    n_fft=256\n",
    "    num_seg = 128\n",
    "    hop_length=len(audio)//num_seg\n",
    "\n",
    "    feature = extract_feature(audio,\n",
    "                             fs,\n",
    "                             segment_size_t,\n",
    "                             n_mfcc,\n",
    "                             n_fft,\n",
    "                             hop_length,\n",
    "                             audio_transfroms=audio_transforms)\n",
    "    # padding or trucate to the same width\n",
    "    feature = width_padding(feature, num_seg)\n",
    "    return feature[None, ...].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fossil-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_config = {\n",
    "    \"do_remove_silent\": True,\n",
    "    \"n_mfcc\": 15,\n",
    "    \"n_fft\": 1024, \n",
    "    \"hop_length\": 256, \n",
    "    \"target_sr\": 48000, \n",
    "    \"max_duration\": 15\n",
    "}\n",
    "audio_transforms = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-indie",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "appropriate-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimmBackbone(nn.Module):\n",
    "    def __init__(self, model_name, inchannels=3, num_classes=1, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(model_name, in_chans=inchannels, pretrained=pretrained)\n",
    "        n_features = self.backbone.num_features\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(n_features, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "    def freeze(self):\n",
    "        # pass\n",
    "        # print(\"freeze feature_extractor\")\n",
    "        for param in self.backbone.parameters():\n",
    "            param.require_grad = False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        # pass\n",
    "        for param in self.backbone.parameters():\n",
    "            param.require_grad = True\n",
    "\n",
    "    def forward(self, x, fp16=False):\n",
    "        with amp.autocast(enabled=fp16):\n",
    "            x = x.float()\n",
    "            feats = self.backbone.forward_features(x)\n",
    "            x = self.pool(feats).view(x.size(0), -1)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.drop(x)\n",
    "            x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "legal-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='tf_efficientnet_b0_ns'\n",
    "inchannels=1\n",
    "num_classes=2\n",
    "pretrained=False\n",
    "\n",
    "model = TimmBackbone(model_name, inchannels=inchannels, num_classes=num_classes, pretrained=pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "tender-hunter",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_paths = []\n",
    "checkpoint_path = '/home/hana/sonnh/Covid19_Cough_Classification/saved/models/11-Covid19-B0/0803_175037/model_best_fold1.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "provincial-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(checkpoint_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "turkish-canadian",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-holocaust",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "duplicate-exposure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1806\n",
      "                           uuid  assessment_result\n",
      "0  iV3Db6t1T8b7c5HQY2TwxIhjbzD3                  0\n",
      "1  AxuYWBN0jFVLINCBqIW5aZmGCdu1                  0\n",
      "2  C5eIsssb9GSkaAgIfsHMHeR6fSh1                  0\n",
      "3  YjbEAECMBIaZKyfqOvWy5DDImUb2                  0\n",
      "4  aGOvk4ji0cVqIzCs1jHnzlw2UEy2                  0\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('/home/hana/sonnh/data/AICovidVN/coswara/info.csv')\n",
    "print(len(df_train))\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "broke-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_dir = '/home/hana/sonnh/data/AICovidVN/aicv115m_public_train_full/aicv115m_final_public_train/train_115M_final_rm_silent_2/'\n",
    "# audio_dir = '/home/hana/sonnh/data/AICovidVN/aicv115m_public_train_full/aicv115m_final_public_train/public_train_audio_files/'\n",
    "audio_dir = '/home/hana/sonnh/data/AICovidVN/coswara/audio_rm_slient/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-setting",
   "metadata": {},
   "source": [
    "## mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "confidential-communication",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 384/900 [01:14<02:13,  3.87it/s]/home/hana/sonnh/env/lib/python3.6/site-packages/librosa/filters.py:239: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  \"Empty filters detected in mel frequency basis. \"\n",
      "100%|██████████| 900/900 [03:02<00:00,  4.94it/s]\n"
     ]
    }
   ],
   "source": [
    "train_predicts = []\n",
    "for uuid in tqdm(list(df_train['uuid'])):\n",
    "    with torch.no_grad():\n",
    "        audio_path = '{}/{}.wav'.format(audio_dir, uuid)\n",
    "        audio, fs  = sf.read(audio_path, dtype=\"float32\")\n",
    "        audio = remove_silent(audio, fs, segment_size_t=0.025)\n",
    "        audio = librosa.resample(audio, fs, 48000)\n",
    "        audio = np.array(audio)\n",
    "#         sf.write('temp.wav', audio, 48000)\n",
    "#         audio, fs  = sf.read('temp.wav', dtype=\"float32\")\n",
    "        \n",
    "        image = extract_mfcc_feature(audio, fs, mfcc_config, audio_transforms, for_test=False)\n",
    "        image = torch.tensor(image).unsqueeze(0)\n",
    "        image = image.to(device)\n",
    "        predict = model(image)\n",
    "        predict = torch.softmax(predict, dim = 1)\n",
    "    train_predicts.append(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-spanish",
   "metadata": {},
   "source": [
    "## melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "soviet-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from  soundfile import SoundFile\n",
    "import glob \n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from skimage.transform import rescale, resize, downscale_local_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "republican-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio2melspec(audio, sr, melspec_config):    \n",
    "    #melspectrogram\n",
    "    # if audio_transfroms is not None:\n",
    "    #     audio, original_sr = audio_transfroms(audio, original_sr)\n",
    "    # n_fft = 512   \n",
    "    # hop_length=int(len(audio)/256)\n",
    "\n",
    "    # mel_spect = librosa.feature.melspectrogram(y=audio, sr=original_sr, n_fft=n_fft, hop_length=hop_length)\n",
    "    # mel_spect = librosa.power_to_db(mel_spect, ref=np.max).astype(np.float32)\n",
    "    # image = mono_to_color(mel_spect)\n",
    "    # # new_img =  cv2.merge([image[:, :128], image[:, 128:256], image[:,256:384]])\n",
    "    # new_img =  cv2.merge([image, image, image])\n",
    "    # new_img = cv2.resize(new_img, (256, 128))\n",
    "\n",
    "    IMAGE_WIDTH = melspec_config.get(\"width\", 448)\n",
    "    IMAGE_HEIGHT = melspec_config.get(\"height\", 448)\n",
    "    n_fft = melspec_config.get(\"n_fft\", 2048)\n",
    "    hop_length = melspec_config.get(\"hop_length\", 'audo')\n",
    "    win_length = n_fft#//2\n",
    "#     sr = melspec_config.get(\"target_sr\", 48000)\n",
    "\n",
    "    if hop_length == 'auto':\n",
    "        hop_length = int((len(audio) - win_length + n_fft) / IMAGE_WIDTH) + 1\n",
    "    else:\n",
    "        # print(hop_length)\n",
    "        raise \"Hop length must be compute with image width\"\n",
    "\n",
    "    spect = np.abs(librosa.stft(y=audio, n_fft=n_fft, hop_length=hop_length, win_length=win_length))\n",
    "    if spect.shape[1] < IMAGE_WIDTH:\n",
    "        #print('too large hop length, len(clip)=', len(clip))\n",
    "        hop_length = hop_length - 1\n",
    "        spect = np.abs(librosa.stft(y=audio, n_fft=n_fft, hop_length=hop_length, win_length=win_length))\n",
    "    if spect.shape[1] > IMAGE_WIDTH:\n",
    "        spect = spect[:, :IMAGE_WIDTH]\n",
    "#     n_mels = IMAGE_HEIGHT // 2\n",
    "    n_mels = IMAGE_HEIGHT\n",
    "    spect = librosa.feature.melspectrogram(S=spect, sr=sr, n_fft=n_fft, n_mels=n_mels, fmin=300, fmax=sr//2)\n",
    "    spect = librosa.power_to_db(spect)\n",
    "    # print(spect.shape)\n",
    "    spect = resize(spect, (IMAGE_HEIGHT, IMAGE_WIDTH), preserve_range=True, anti_aliasing=True)\n",
    "    spect = spect - spect.min()\n",
    "    smax = spect.max()\n",
    "\n",
    "    if smax >= 0.001:\n",
    "        spect = spect / smax\n",
    "    else:\n",
    "        spect[...] = 0\n",
    "\n",
    "    return spect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "rolled-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "melspec_config = {\n",
    "    \"width\": 448, \"height\":448, \"n_fft\": 2048, \"hop_length\": \"auto\", \"target_sr\": 48000, \"max_duration\": 15\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "imported-showcase",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>subject_age</th>\n",
       "      <th>subject_gender</th>\n",
       "      <th>audio_noise_note</th>\n",
       "      <th>cough_intervals</th>\n",
       "      <th>assessment_result</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration_type</th>\n",
       "      <th>subject_age_type</th>\n",
       "      <th>noise_type</th>\n",
       "      <th>cough_interval_type</th>\n",
       "      <th>labels</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9341db3f-049a-4ceb-8438-87ca1618a18a</td>\n",
       "      <td>group_34_48</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'start': 1.9051594202898552, 'end': 2.414901...</td>\n",
       "      <td>0</td>\n",
       "      <td>22050</td>\n",
       "      <td>6.741361</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0_5_1_3_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ff8c21a8-4d05-43d8-96ee-dd33bcd6461e</td>\n",
       "      <td>group_19_33</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'end': 1.516455486542442, 'start': 1.1739296...</td>\n",
       "      <td>0</td>\n",
       "      <td>22050</td>\n",
       "      <td>4.010703</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0_1_1_2_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9a55aef7-ed77-45ab-976e-411aade1c783</td>\n",
       "      <td>group_19_33</td>\n",
       "      <td>male</td>\n",
       "      <td>tiếng nói chuyện, trao đổi</td>\n",
       "      <td>[{'start': 2.393298134459697, 'end': 2.7450841...</td>\n",
       "      <td>0</td>\n",
       "      <td>48000</td>\n",
       "      <td>5.461333</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0_3_0_2_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bf19a7f1-fdff-42c7-a99d-40546b39f745</td>\n",
       "      <td>group_65_78</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'end': 1.47575373406193, 'start': 0.69777486...</td>\n",
       "      <td>1</td>\n",
       "      <td>22050</td>\n",
       "      <td>7.338685</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1_7_1_5_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1c2e03d2-1285-42ed-bca2-b3f1ab94aa6b</td>\n",
       "      <td>group_65_78</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'end': 1.262585034013605, 'start': 0.9351079...</td>\n",
       "      <td>0</td>\n",
       "      <td>48000</td>\n",
       "      <td>2.133333</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0_0_1_5_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4468</th>\n",
       "      <td>8e98b0ef-d164-4fc3-b28b-24f5b70b33c4</td>\n",
       "      <td>group_14_18</td>\n",
       "      <td>female</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'start': 0.8291162349457271, 'end': 1.325240...</td>\n",
       "      <td>0</td>\n",
       "      <td>48000</td>\n",
       "      <td>3.669333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0_1_1_1_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>e7a7732c-0085-499c-9bd8-13c3de35741e</td>\n",
       "      <td>group_19_33</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'start': 1.034520547945205, 'end': 1.2332420...</td>\n",
       "      <td>0</td>\n",
       "      <td>48000</td>\n",
       "      <td>5.546667</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0_3_1_2_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4494</th>\n",
       "      <td>24ac8a4c-b2a2-4dd0-89ac-7ff8801e9558</td>\n",
       "      <td>group_19_33</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'end': 2.5490455813953483, 'start': 1.668881...</td>\n",
       "      <td>0</td>\n",
       "      <td>48000</td>\n",
       "      <td>6.144000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0_4_1_2_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>f744cd1c-8f0d-4043-b800-19e39d443161</td>\n",
       "      <td>group_19_33</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'start': 1.9232463768115942, 'end': 2.529590...</td>\n",
       "      <td>0</td>\n",
       "      <td>48000</td>\n",
       "      <td>12.202667</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0_7_1_2_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500</th>\n",
       "      <td>c936f1a1-f6cf-48e5-82a1-872a58b6fd01</td>\n",
       "      <td>group_19_33</td>\n",
       "      <td>female</td>\n",
       "      <td>không có âm thanh</td>\n",
       "      <td>[{'start': 0.204485339942542, 'end': 0.3904662...</td>\n",
       "      <td>1</td>\n",
       "      <td>22050</td>\n",
       "      <td>0.847574</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1_0_0_2_2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>901 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      uuid  subject_age subject_gender  \\\n",
       "1     9341db3f-049a-4ceb-8438-87ca1618a18a  group_34_48           male   \n",
       "3     ff8c21a8-4d05-43d8-96ee-dd33bcd6461e  group_19_33           male   \n",
       "7     9a55aef7-ed77-45ab-976e-411aade1c783  group_19_33           male   \n",
       "21    bf19a7f1-fdff-42c7-a99d-40546b39f745  group_65_78           male   \n",
       "29    1c2e03d2-1285-42ed-bca2-b3f1ab94aa6b  group_65_78           male   \n",
       "...                                    ...          ...            ...   \n",
       "4468  8e98b0ef-d164-4fc3-b28b-24f5b70b33c4  group_14_18         female   \n",
       "4481  e7a7732c-0085-499c-9bd8-13c3de35741e  group_19_33           male   \n",
       "4494  24ac8a4c-b2a2-4dd0-89ac-7ff8801e9558  group_19_33           male   \n",
       "4496  f744cd1c-8f0d-4043-b800-19e39d443161  group_19_33           male   \n",
       "4500  c936f1a1-f6cf-48e5-82a1-872a58b6fd01  group_19_33         female   \n",
       "\n",
       "                audio_noise_note  \\\n",
       "1                           None   \n",
       "3                           None   \n",
       "7     tiếng nói chuyện, trao đổi   \n",
       "21                          None   \n",
       "29                          None   \n",
       "...                          ...   \n",
       "4468                        None   \n",
       "4481                        None   \n",
       "4494                        None   \n",
       "4496                        None   \n",
       "4500           không có âm thanh   \n",
       "\n",
       "                                        cough_intervals  assessment_result  \\\n",
       "1     [{'start': 1.9051594202898552, 'end': 2.414901...                  0   \n",
       "3     [{'end': 1.516455486542442, 'start': 1.1739296...                  0   \n",
       "7     [{'start': 2.393298134459697, 'end': 2.7450841...                  0   \n",
       "21    [{'end': 1.47575373406193, 'start': 0.69777486...                  1   \n",
       "29    [{'end': 1.262585034013605, 'start': 0.9351079...                  0   \n",
       "...                                                 ...                ...   \n",
       "4468  [{'start': 0.8291162349457271, 'end': 1.325240...                  0   \n",
       "4481  [{'start': 1.034520547945205, 'end': 1.2332420...                  0   \n",
       "4494  [{'end': 2.5490455813953483, 'start': 1.668881...                  0   \n",
       "4496  [{'start': 1.9232463768115942, 'end': 2.529590...                  0   \n",
       "4500  [{'start': 0.204485339942542, 'end': 0.3904662...                  1   \n",
       "\n",
       "      sample_rate   duration  duration_type  subject_age_type  noise_type  \\\n",
       "1           22050   6.741361              5                 3           1   \n",
       "3           22050   4.010703              1                 2           1   \n",
       "7           48000   5.461333              3                 2           0   \n",
       "21          22050   7.338685              7                 5           1   \n",
       "29          48000   2.133333              0                 5           1   \n",
       "...           ...        ...            ...               ...         ...   \n",
       "4468        48000   3.669333              1                 1           1   \n",
       "4481        48000   5.546667              3                 2           1   \n",
       "4494        48000   6.144000              4                 2           1   \n",
       "4496        48000  12.202667              7                 2           1   \n",
       "4500        22050   0.847574              0                 2           0   \n",
       "\n",
       "      cough_interval_type     labels  fold  \n",
       "1                       2  0_5_1_3_2   1.0  \n",
       "3                       2  0_1_1_2_2   1.0  \n",
       "7                       2  0_3_0_2_2   1.0  \n",
       "21                      2  1_7_1_5_2   1.0  \n",
       "29                      2  0_0_1_5_2   1.0  \n",
       "...                   ...        ...   ...  \n",
       "4468                    2  0_1_1_1_2   1.0  \n",
       "4481                    2  0_3_1_2_2   1.0  \n",
       "4494                    2  0_4_1_2_2   1.0  \n",
       "4496                    2  0_7_1_2_2   1.0  \n",
       "4500                    2  1_0_0_2_2   1.0  \n",
       "\n",
       "[901 rows x 14 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "pretty-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "image_transform = albu.Compose([\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "max_samples = 48000 * 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "understanding-foster",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/901 [00:00<?, ?it/s]/home/hana/sonnh/env/lib/python3.6/site-packages/librosa/filters.py:239: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  \"Empty filters detected in mel frequency basis. \"\n",
      "/home/hana/sonnh/env/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 901/901 [03:02<00:00,  4.93it/s]\n"
     ]
    }
   ],
   "source": [
    "train_predicts = []\n",
    "list_uuid = list(df_train['uuid'])\n",
    "for uuid in tqdm(list_uuid):\n",
    "    with torch.no_grad():\n",
    "        audio_path = '{}/{}.wav'.format(audio_dir, uuid)\n",
    "        audio, fs  = sf.read(audio_path, dtype=\"float32\")\n",
    "        audio = remove_silent(audio, fs, segment_size_t=0.025)\n",
    "        audio = librosa.resample(audio, fs, 48000)\n",
    "        audio = np.array(audio)\n",
    "        \n",
    "        \n",
    "        sf.write('temp.wav', audio, 48000)\n",
    "        audio, fs  = sf.read('temp.wav', dtype=\"float32\")\n",
    "        \n",
    "        audio = trim_and_pad(audio, max_samples)\n",
    "        image = audio2melspec(audio, fs, melspec_config)\n",
    "        image = image_transform(image = image)['image']\n",
    "        image = torch.tensor(image).unsqueeze(0)\n",
    "        image = image.to(device)\n",
    "        predict = model(image)\n",
    "        predict = torch.softmax(predict, dim = 1)\n",
    "    train_predicts.append(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-viking",
   "metadata": {},
   "source": [
    "## metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "limited-hammer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = torch.tensor(list(df_train['assessment_result']))\n",
    "train_predicts = torch.cat(train_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "textile-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predicts = train_predicts.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ordinary-marijuana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9846658004356632"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_multi_label(train_predicts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "located-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predicts_1 = train_predicts[:, 1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "threatened-nurse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9153680064323205"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(labels, train_predicts_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "exact-shade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([758, 143])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_predicts>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "perfect-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predicts_1_r = np.array(train_predicts_1) >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "complimentary-batch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.888"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.recall_score(labels, train_predicts_1_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "departmental-racing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(labels).count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "acquired-defeat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(labels).count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "mobile-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = sum(np.bitwise_and(labels, train_predicts_1_r))\n",
    "tn = len(train_predicts_1_r) - sum(np.bitwise_or(labels, train_predicts_1_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "swiss-league",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(111)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "sharp-drawing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_predicts_1_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "clear-cooling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(753)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "swedish-radar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9148936170212766"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.precision_score(labels, train_predicts_1_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-jurisdiction",
   "metadata": {},
   "source": [
    "# test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "first-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_test_dir = '/home/hana/sonnh/data/AICovidVN/aicv115m_public_train_full/aicv115m_final_public_test/public_test_audio_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fourth-poverty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>assessment_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7b3797b0-3b7e-41e3-8b28-e2717eb55f8b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0c466b3-7bf2-47e4-9e7f-f8cfc1783764</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a2d668e9-d876-4bf6-bcb3-0cc32ba20c84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0edbea61-da70-44a4-8ee8-3681027944a6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1bcee200-1c33-4293-b1e9-5854210d92e8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>ab3f935b-3056-4a28-aa88-5823cfb0d30d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>bd8f4a34-33aa-40ad-a390-eb6bc9f04475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>ba5f136a-6c8e-4671-8d1a-5aeaf217bbc7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>634af752-a1ae-424d-b14c-cb2950950cac</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>2efb787e-9806-4059-bca4-d78a001c12a5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1233 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      uuid  assessment_result\n",
       "0     7b3797b0-3b7e-41e3-8b28-e2717eb55f8b                  0\n",
       "1     f0c466b3-7bf2-47e4-9e7f-f8cfc1783764                  0\n",
       "2     a2d668e9-d876-4bf6-bcb3-0cc32ba20c84                  0\n",
       "3     0edbea61-da70-44a4-8ee8-3681027944a6                  0\n",
       "4     1bcee200-1c33-4293-b1e9-5854210d92e8                  0\n",
       "...                                    ...                ...\n",
       "1228  ab3f935b-3056-4a28-aa88-5823cfb0d30d                  0\n",
       "1229  bd8f4a34-33aa-40ad-a390-eb6bc9f04475                  0\n",
       "1230  ba5f136a-6c8e-4671-8d1a-5aeaf217bbc7                  0\n",
       "1231  634af752-a1ae-424d-b14c-cb2950950cac                  0\n",
       "1232  2efb787e-9806-4059-bca4-d78a001c12a5                  0\n",
       "\n",
       "[1233 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('/home/hana/sonnh/data/AICovidVN/aicv115m_public_train_full/aicv115m_final_public_test/public_test_sample_submission.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "indonesian-anniversary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1233/1233 [04:50<00:00,  4.25it/s]\n"
     ]
    }
   ],
   "source": [
    "predicts = []\n",
    "for uuid in tqdm(list(df_test['uuid'])):\n",
    "    with torch.no_grad():\n",
    "        audio_path = '{}/{}.wav'.format(audio_test_dir, uuid)\n",
    "        audio, fs  = sf.read(audio_path, dtype=\"float32\")\n",
    "        audio = remove_silent(audio, fs, segment_size_t=0.025)\n",
    "        audio = librosa.resample(audio, fs, 48000)\n",
    "        \n",
    "        sf.write('temp.wav', audio, 48000)\n",
    "        audio, fs  = sf.read('temp.wav', dtype=\"float32\")\n",
    "\n",
    "        image = extract_mfcc_feature(audio, fs, mfcc_config, audio_transforms, for_test=False)\n",
    "        image = torch.tensor(image).unsqueeze(0)\n",
    "        image = image.to(device)\n",
    "        predict = model(image)\n",
    "        predict = torch.softmax(predict, dim = 1)\n",
    "    predicts.append(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-setup",
   "metadata": {},
   "source": [
    "## melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "generic-salon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1233 [00:00<?, ?it/s]/home/hana/sonnh/env/lib/python3.6/site-packages/librosa/filters.py:239: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  \"Empty filters detected in mel frequency basis. \"\n",
      "/home/hana/sonnh/env/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 1233/1233 [04:28<00:00,  4.60it/s]\n"
     ]
    }
   ],
   "source": [
    "predicts = []\n",
    "for uuid in tqdm(list(df_test['uuid'])):\n",
    "    with torch.no_grad():\n",
    "        audio_path = '{}/{}.wav'.format(audio_test_dir, uuid)\n",
    "        audio, fs  = sf.read(audio_path, dtype=\"float32\")\n",
    "        audio = remove_silent(audio, fs, segment_size_t=0.025)\n",
    "        audio = librosa.resample(audio, fs, 48000)\n",
    "        audio = np.array(audio)\n",
    "        \n",
    "        \n",
    "        sf.write('temp.wav', audio, 48000)\n",
    "        audio, fs  = sf.read('temp.wav', dtype=\"float32\")\n",
    "        \n",
    "        audio = trim_and_pad(audio, max_samples)\n",
    "        image = audio2melspec(audio, fs, melspec_config)\n",
    "        image = image_transform(image = image)['image']\n",
    "        image = torch.tensor(image).unsqueeze(0)\n",
    "        image = image.to(device)\n",
    "        predict = model(image)\n",
    "        predict = torch.softmax(predict, dim = 1)\n",
    "    predicts.append(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "simplified-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = torch.cat(predicts).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "medical-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predicts = predicts[:, 1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fabulous-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['assessment_result'] = final_predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "otherwise-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('/home/hana/sonnh/covid19_res/11_b0_fold1_fix_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "suburban-fireplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asilla/sonnh/covid\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "tribal-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predicts = np.array(final_predicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "recreational-chosen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final_predicts>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "optical-investigator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>assessment_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7b3797b0-3b7e-41e3-8b28-e2717eb55f8b</td>\n",
       "      <td>0.194483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0c466b3-7bf2-47e4-9e7f-f8cfc1783764</td>\n",
       "      <td>0.020574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a2d668e9-d876-4bf6-bcb3-0cc32ba20c84</td>\n",
       "      <td>0.016501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0edbea61-da70-44a4-8ee8-3681027944a6</td>\n",
       "      <td>0.013703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1bcee200-1c33-4293-b1e9-5854210d92e8</td>\n",
       "      <td>0.022322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid  assessment_result\n",
       "0  7b3797b0-3b7e-41e3-8b28-e2717eb55f8b           0.194483\n",
       "1  f0c466b3-7bf2-47e4-9e7f-f8cfc1783764           0.020574\n",
       "2  a2d668e9-d876-4bf6-bcb3-0cc32ba20c84           0.016501\n",
       "3  0edbea61-da70-44a4-8ee8-3681027944a6           0.013703\n",
       "4  1bcee200-1c33-4293-b1e9-5854210d92e8           0.022322"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-memory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-bride",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "mechanical-baking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hana/sonnh/data/AICovidVN/aicv115m_public_train_full/aicv115m_final_public_train/public_train_audio_files//f616cb8d-370f-43e6-b459-121a9b987c94.wav'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "upper-worcester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0000000e+00 3.0517578e-05 3.0517578e-05 ... 5.7983398e-04 6.1035156e-04\n",
      " 6.1035156e-04]\n"
     ]
    }
   ],
   "source": [
    "audio_path = '{}/{}.wav'.format(audio_test_dir, uuid)\n",
    "audio, fs  = sf.read(audio_path, dtype=\"float32\")\n",
    "audio = remove_silent(audio, fs, segment_size_t=0.025)\n",
    "audio = librosa.resample(audio, fs, 48000)\n",
    "\n",
    "print(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "aerial-trick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 3.0517578e-05, 3.0517578e-05, ..., 5.7983398e-04,\n",
       "       6.1035156e-04, 6.1035156e-04], dtype=float32)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "wound-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write('temp.wav', audio, 48000)\n",
    "audio, fs  = sf.read('temp.wav', dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "facial-confirmation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 == 'd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-identifier",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
